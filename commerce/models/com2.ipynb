{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Commerce Challenge 2 - Baseline Submission\n",
        "\n",
        "This notebook provides a simple baseline for **Commerce Challenge 2: Personalized Product Recommendations**.\n",
        "\n",
        "**Goal**: Recommend top-10 SKUs for each customer (wide format)\n",
        "**Metric**: NDCG@10 - Higher is better\n",
        "\n",
        "## Instructions:\n",
        "1. **Replace API credentials** in the first cell with your team's API key and name\n",
        "2. **Run all cells** to generate and submit baseline predictions\n",
        "3. **Check the output** for your submission score\n",
        "\n",
        "This baseline uses a simple popularity-based recommender system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“‚ Loading Commerce Challenge 2 data...\n",
            "âœ… Data loaded:\n",
            "   Train purchases: (16616, 4)\n",
            "   Test purchases: (500, 2)\n",
            "   Train columns: ['customer_id', 'order_id', 'month', 'sku_id']\n",
            "   Test columns: ['customer_id', 'month']\n"
          ]
        }
      ],
      "source": [
        "# 1. Initialize Client and Load Data\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from agentds import BenchmarkClient\n",
        "\n",
        "# ðŸ”‘ REPLACE WITH YOUR CREDENTIALS\n",
        "# ðŸ”‘ REPLACE WITH YOUR CREDENTIALS\n",
        "client = BenchmarkClient(\n",
        "    api_key=\"your-api-key-here\",        # Get from your team dashboard\n",
        "    team_name=\"your-team-name-here\"     # Your exact team name\n",
        ")\n",
        "\n",
        "# Load data from PVC paths\n",
        "print(\"ðŸ“‚ Loading Commerce Challenge 2 data...\")\n",
        "\n",
        "# Load purchase history data\n",
        "train_purchases = pd.read_csv(\"../Commerce/purchases_train.csv\")\n",
        "test_purchases = pd.read_csv(\"../Commerce/purchases_test.csv\")\n",
        "\n",
        "print(f\"âœ… Data loaded:\")\n",
        "print(f\"   Train purchases: {train_purchases.shape}\")\n",
        "print(f\"   Test purchases: {test_purchases.shape}\")\n",
        "print(f\"   Train columns: {list(train_purchases.columns)}\")\n",
        "print(f\"   Test columns: {list(test_purchases.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“Š Filtered training data: 16616 purchases for 750 valid SKUs\n",
            "âœ… Built frequency table: 14462 customer-SKU pairs\n",
            "ðŸŽ¯ Need to recommend for 500 customers\n",
            "âœ… Predictions saved: 500 customers\n",
            "   Preview:    customer_id  sku_id_1  sku_id_2  sku_id_3  sku_id_4  sku_id_5  sku_id_6  \\\n",
            "0            1       615       611       632       648       584       570   \n",
            "1            2        30       443       188       431       405       140   \n",
            "2            3       387       213        56      1247       138       173   \n",
            "\n",
            "   sku_id_7  sku_id_8  sku_id_9  sku_id_10  \n",
            "0       562       595       633        697  \n",
            "1       529        76       432        467  \n",
            "2       328      1211       366        321  \n",
            "   Format: Personalized frequency-based recommendations\n"
          ]
        }
      ],
      "source": [
        "# 2. Frequency-Based Baseline (Personalized)\n",
        "\n",
        "# Load products_sub to get valid SKUs for Challenge 2\n",
        "products_sub = pd.read_csv(\"/Users/amolighatimma/Desktop/AgentDS/Commerce/products_sub.csv\")\n",
        "valid_skus = set(products_sub['sku_id'].tolist())\n",
        "\n",
        "# Filter train_purchases to only include valid SKUs\n",
        "train_filtered = train_purchases[train_purchases['sku_id'].isin(valid_skus)]\n",
        "\n",
        "print(f\"ðŸ“Š Filtered training data: {train_filtered.shape[0]} purchases for {len(valid_skus)} valid SKUs\")\n",
        "\n",
        "# 1. Per-customer SKU frequency table\n",
        "freq_tbl = (train_filtered\n",
        "            .groupby([\"customer_id\", \"sku_id\"])\n",
        "            .size()\n",
        "            .rename(\"cnt\")\n",
        "            .reset_index())\n",
        "\n",
        "# 2. Global popularity for tie-breaking\n",
        "glob_pop = (train_filtered[\"sku_id\"]\n",
        "            .value_counts()\n",
        "            .rename_axis(\"sku_id\")\n",
        "            .rename(\"glob_cnt\")\n",
        "            .reset_index())\n",
        "\n",
        "# Merge global popularity into freq table\n",
        "freq_tbl = freq_tbl.merge(glob_pop, on=\"sku_id\", how=\"left\")\n",
        "\n",
        "print(f\"âœ… Built frequency table: {len(freq_tbl)} customer-SKU pairs\")\n",
        "\n",
        "# Get test customers who need recommendations\n",
        "test_customers = test_purchases['customer_id'].unique()\n",
        "print(f\"ðŸŽ¯ Need to recommend for {len(test_customers)} customers\")\n",
        "\n",
        "# Create personalized recommendations\n",
        "recommendations = []\n",
        "\n",
        "for customer_id in test_customers:\n",
        "    # Get customer's frequency scores\n",
        "    customer_freq = freq_tbl[freq_tbl['customer_id'] == customer_id].copy()\n",
        "    \n",
        "    if len(customer_freq) > 0:\n",
        "        # Customer has purchase history - use frequency + global popularity\n",
        "        customer_freq['score'] = customer_freq['cnt'] + 0.0001 * customer_freq['glob_cnt']\n",
        "        recommended_skus = customer_freq.nlargest(10, 'score')['sku_id'].tolist()\n",
        "    else:\n",
        "        # New customer - use global popularity\n",
        "        recommended_skus = glob_pop.head(10)['sku_id'].tolist()\n",
        "    \n",
        "    # Ensure we have exactly 10 recommendations\n",
        "    if len(recommended_skus) < 10:\n",
        "        # Fill remaining with other popular SKUs\n",
        "        remaining_skus = [sku for sku in glob_pop['sku_id'].tolist() if sku not in recommended_skus]\n",
        "        recommended_skus.extend(remaining_skus[:10-len(recommended_skus)])\n",
        "    \n",
        "    recommended_skus = recommended_skus[:10]\n",
        "    \n",
        "    # Create row: customer_id, sku_id_1, sku_id_2, ..., sku_id_10\n",
        "    row = [customer_id] + recommended_skus\n",
        "    recommendations.append(row)\n",
        "\n",
        "# Create submission DataFrame in wide format\n",
        "columns = ['customer_id'] + [f'sku_id_{i}' for i in range(1, 11)]\n",
        "submission_df = pd.DataFrame(recommendations, columns=columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/amolighatimma/anaconda3/lib/python3.11/site-packages/implicit/utils.py:164: ParameterWarning: Method expects CSR input, and was passed csc_matrix instead. Converting to CSR took 0.0001628398895263672 seconds\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "589594a49ace427a9517b966c0a9195b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ALS top-10 recommendations ready (stable)!\n",
            "   customer_id  sku_id_1  sku_id_2  sku_id_3  sku_id_4  sku_id_5  sku_id_6  \\\n",
            "0            1        10      1246        56       907      1233       312   \n",
            "1            2       533      1233       268       907      1123      1129   \n",
            "2            3      1233      1061       206       533       268      1155   \n",
            "3            4       533      1123      1129       268       907        10   \n",
            "4            5      1129       533        10      1020      1233       496   \n",
            "\n",
            "   sku_id_7  sku_id_8  sku_id_9  sku_id_10  \n",
            "0       533      1129       809        974  \n",
            "1        56       636        10       1246  \n",
            "2      1020       907      1301       1109  \n",
            "3       562       312       636       1233  \n",
            "4        56       333       411       1123  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix\n",
        "from implicit.als import AlternatingLeastSquares\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Load Data\n",
        "# -----------------------------\n",
        "train_purchases = pd.read_csv(\"../Commerce/purchases_train.csv\")\n",
        "test_purchases = pd.read_csv(\"../Commerce/purchases_test.csv\")\n",
        "products_sub = pd.read_csv(\"/Users/amolighatimma/Desktop/AgentDS/Commerce/products_sub.csv\")\n",
        "\n",
        "valid_skus = set(products_sub['sku_id'].tolist())\n",
        "train_filtered = train_purchases[train_purchases['sku_id'].isin(valid_skus)]\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Map IDs to indices\n",
        "# -----------------------------\n",
        "customer_ids = train_filtered['customer_id'].unique()\n",
        "sku_ids = train_filtered['sku_id'].unique()\n",
        "customer2idx = {cid: i for i, cid in enumerate(customer_ids)}\n",
        "sku2idx = {sid: i for i, sid in enumerate(sku_ids)}\n",
        "idx2sku = {i: sid for sid, i in sku2idx.items()}\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Build interaction matrix\n",
        "# -----------------------------\n",
        "rows = train_filtered['customer_id'].map(customer2idx)\n",
        "cols = train_filtered['sku_id'].map(sku2idx)\n",
        "data = np.ones(len(train_filtered))  # implicit feedback\n",
        "interaction_matrix = coo_matrix((data, (rows, cols)), shape=(len(customer_ids), len(sku_ids)))\n",
        "interaction_matrix = interaction_matrix.tocsr()\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Train ALS model\n",
        "# -----------------------------\n",
        "als_model = AlternatingLeastSquares(\n",
        "    factors=64,\n",
        "    regularization=0.1,\n",
        "    iterations=30,\n",
        "    use_gpu=False\n",
        ")\n",
        "als_model.fit(interaction_matrix.T)  # items x users\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Top global SKUs for cold-start\n",
        "# -----------------------------\n",
        "top_global_skus = train_filtered['sku_id'].value_counts().head(10).index.tolist()\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Generate top-10 recommendations\n",
        "# -----------------------------\n",
        "test_customers = test_purchases['customer_id'].unique()\n",
        "recommendations = []\n",
        "\n",
        "for cust in test_customers:\n",
        "    if cust in customer2idx:\n",
        "        cust_idx = customer2idx[cust]\n",
        "        \n",
        "        # Option 2: manual scoring (stable)\n",
        "        user_vector = als_model.user_factors[cust_idx]       # 1 x latent factors\n",
        "        scores = als_model.item_factors @ user_vector       # score for each item\n",
        "        top_idx = np.argsort(-scores)[:10]                  # top-10 indices\n",
        "        recommended_skus = [idx2sku[i] for i in top_idx]\n",
        "\n",
        "    else:\n",
        "        # Cold-start: top global SKUs\n",
        "        recommended_skus = top_global_skus.copy()\n",
        "\n",
        "    # Ensure exactly 10 SKUs\n",
        "    if len(recommended_skus) < 10:\n",
        "        remaining_skus = [sku for sku in top_global_skus if sku not in recommended_skus]\n",
        "        recommended_skus.extend(remaining_skus[:10 - len(recommended_skus)])\n",
        "\n",
        "    recommended_skus = recommended_skus[:10]\n",
        "    recommendations.append([cust] + recommended_skus)\n",
        "\n",
        "# -----------------------------\n",
        "# 7. Create submission CSV\n",
        "# -----------------------------\n",
        "columns = ['customer_id'] + [f'sku_id_{i}' for i in range(1, 11)]\n",
        "submission_df = pd.DataFrame(recommendations, columns=columns)\n",
        "submission_df.to_csv(\"commerce_challenge2_predictions.csv\", index=False)\n",
        "\n",
        "print(\"âœ… ALS top-10 recommendations ready (stable)!\")\n",
        "print(submission_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below marks the shift from using ALS models to anti-recommendation theory being integrated into the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŽ¯ Anti-recommendation strategy...\n",
            "âœ… Anti-bad predictions saved: 500 customers\n"
          ]
        }
      ],
      "source": [
        "# IDEA 1: IDENTIFY AND REMOVE BAD RECOMMENDATIONS\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_purchases = pd.read_csv(\"../Commerce/purchases_train.csv\")\n",
        "test_purchases = pd.read_csv(\"../Commerce/purchases_test.csv\")\n",
        "products_sub = pd.read_csv(\"/Users/amolighatimma/Desktop/AgentDS/Commerce/products_sub.csv\")\n",
        "\n",
        "valid_skus = set(products_sub['sku_id'].tolist())\n",
        "train_filtered = train_purchases[train_purchases['sku_id'].isin(valid_skus)]\n",
        "\n",
        "print(\"ðŸŽ¯ Anti-recommendation strategy...\")\n",
        "\n",
        "# Your working 0.0769 approach\n",
        "freq_tbl = (train_filtered.groupby([\"customer_id\", \"sku_id\"])\n",
        "            .size().rename(\"cnt\").reset_index())\n",
        "\n",
        "glob_pop = (train_filtered[\"sku_id\"].value_counts()\n",
        "            .rename_axis(\"sku_id\").rename(\"glob_cnt\").reset_index())\n",
        "glob_pop['glob_score'] = np.log1p(glob_pop['glob_cnt'])\n",
        "\n",
        "freq_tbl = freq_tbl.merge(glob_pop[['sku_id', 'glob_score']], on='sku_id', how='left')\n",
        "freq_tbl['glob_score'] = freq_tbl['glob_score'].fillna(0)\n",
        "\n",
        "# NEW: Identify potentially BAD recommendations\n",
        "# Items that customers buy once but never again (buyer's remorse)\n",
        "one_time_purchases = train_filtered.groupby(['customer_id', 'sku_id']).size().reset_index(name='cnt')\n",
        "one_time_purchases = one_time_purchases[one_time_purchases['cnt'] == 1]\n",
        "\n",
        "# Items that are rarely bought by the same customer twice\n",
        "repeat_purchase_rate = train_filtered.groupby('sku_id').agg({\n",
        "    'customer_id': ['count', 'nunique']\n",
        "}).reset_index()\n",
        "repeat_purchase_rate.columns = ['sku_id', 'total_purchases', 'unique_customers']\n",
        "repeat_purchase_rate['repeat_rate'] = repeat_purchase_rate['total_purchases'] / repeat_purchase_rate['unique_customers']\n",
        "\n",
        "# Penalize items with low repeat rates (customers don't buy them again)\n",
        "repeat_purchase_rate['penalty'] = np.where(\n",
        "    repeat_purchase_rate['repeat_rate'] < 1.1,  # Less than 10% repeat purchases\n",
        "    0.5,  # Penalty\n",
        "    1.0   # No penalty\n",
        ")\n",
        "\n",
        "freq_tbl = freq_tbl.merge(repeat_purchase_rate[['sku_id', 'penalty']], on='sku_id', how='left')\n",
        "freq_tbl['penalty'] = freq_tbl['penalty'].fillna(1.0)\n",
        "\n",
        "test_customers = test_purchases['customer_id'].unique()\n",
        "recommendations = []\n",
        "\n",
        "for customer_id in test_customers:\n",
        "    customer_freq = freq_tbl[freq_tbl['customer_id'] == customer_id].copy()\n",
        "    \n",
        "    if len(customer_freq) > 0:\n",
        "        # Apply penalty to potentially bad recommendations\n",
        "        customer_freq['score'] = (customer_freq['cnt'] + 0.05 * customer_freq['glob_score']) * customer_freq['penalty']\n",
        "        recommended_skus = customer_freq.nlargest(10, 'score')['sku_id'].tolist()\n",
        "    else:\n",
        "        recommended_skus = glob_pop.head(10)['sku_id'].tolist()\n",
        "    \n",
        "    if len(recommended_skus) < 10:\n",
        "        remaining_skus = [sku for sku in glob_pop['sku_id'].tolist() if sku not in recommended_skus]\n",
        "        recommended_skus.extend(remaining_skus[:10-len(recommended_skus)])\n",
        "    \n",
        "    recommendations.append([customer_id] + recommended_skus[:10])\n",
        "\n",
        "columns = ['customer_id'] + [f'sku_id_{i}' for i in range(1, 11)]\n",
        "submission_df = pd.DataFrame(recommendations, columns=columns)\n",
        "submission_df.to_csv(\"commerce_challenge2_predictions_ar.csv\", index=False)\n",
        "\n",
        "print(f\"âœ… Anti-bad predictions saved: {submission_df.shape[0]} customers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŽ¯ Enhanced anti-recommendation v2...\n",
            "âœ… Enhanced anti-bad v2 predictions saved: 500 customers\n",
            "ðŸŽ¯ Expected: 0.0775 - 0.0785\n"
          ]
        }
      ],
      "source": [
        "# ENHANCED ANTI-RECOMMENDATION v2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_purchases = pd.read_csv(\"../Commerce/purchases_train.csv\")\n",
        "test_purchases = pd.read_csv(\"../Commerce/purchases_test.csv\")\n",
        "products_sub = pd.read_csv(\"/Users/amolighatimma/Desktop/AgentDS/Commerce/products_sub.csv\")\n",
        "\n",
        "valid_skus = set(products_sub['sku_id'].tolist())\n",
        "train_filtered = train_purchases[train_purchases['sku_id'].isin(valid_skus)]\n",
        "\n",
        "print(\"ðŸŽ¯ Enhanced anti-recommendation v2...\")\n",
        "\n",
        "# Your working 0.0769 approach\n",
        "freq_tbl = (train_filtered.groupby([\"customer_id\", \"sku_id\"])\n",
        "            .size().rename(\"cnt\").reset_index())\n",
        "\n",
        "glob_pop = (train_filtered[\"sku_id\"].value_counts()\n",
        "            .rename_axis(\"sku_id\").rename(\"glob_cnt\").reset_index())\n",
        "glob_pop['glob_score'] = np.log1p(glob_pop['glob_cnt'])\n",
        "\n",
        "freq_tbl = freq_tbl.merge(glob_pop[['sku_id', 'glob_score']], on='sku_id', how='left')\n",
        "freq_tbl['glob_score'] = freq_tbl['glob_score'].fillna(0)\n",
        "\n",
        "# ENHANCED BAD RECOMMENDATION DETECTION\n",
        "\n",
        "# 1. Items with low repeat rates (buyer's remorse)\n",
        "repeat_purchase_rate = train_filtered.groupby('sku_id').agg({\n",
        "    'customer_id': ['count', 'nunique']\n",
        "}).reset_index()\n",
        "repeat_purchase_rate.columns = ['sku_id', 'total_purchases', 'unique_customers']\n",
        "repeat_purchase_rate['repeat_rate'] = repeat_purchase_rate['total_purchases'] / repeat_purchase_rate['unique_customers']\n",
        "\n",
        "# Stronger penalty for low repeat rates\n",
        "repeat_purchase_rate['penalty_repeat'] = np.where(\n",
        "    repeat_purchase_rate['repeat_rate'] < 1.05, 0.3,  # Very low repeat = strong penalty\n",
        "    np.where(repeat_purchase_rate['repeat_rate'] < 1.2, 0.7,  # Low repeat = medium penalty  \n",
        "    1.0)  # Good repeat rate = no penalty\n",
        ")\n",
        "\n",
        "# 2. Items that are \"too popular\" (might be too generic)\n",
        "popularity_penalty = np.where(\n",
        "    glob_pop['glob_cnt'] > glob_pop['glob_cnt'].quantile(0.8),  # Top 20% most popular\n",
        "    0.8,  # Slight penalty for ultra-popular items\n",
        "    1.0   # No penalty for regular items\n",
        ")\n",
        "glob_pop['penalty_popularity'] = popularity_penalty\n",
        "\n",
        "# 3. Items that customers quickly stop buying (if we have temporal data)\n",
        "if 'month' in train_filtered.columns:\n",
        "    # Find items where customers don't repurchase in subsequent months\n",
        "    customer_sequences = train_filtered.sort_values(['customer_id', 'month'])\n",
        "    customer_sequences['next_purchase_month'] = customer_sequences.groupby('customer_id')['month'].shift(-1)\n",
        "    customer_sequences['months_until_next'] = customer_sequences['next_purchase_month'] - customer_sequences['month']\n",
        "    \n",
        "    # Items with long gaps before repurchase (or no repurchase)\n",
        "    repurchase_gaps = customer_sequences.groupby('sku_id')['months_until_next'].mean().reset_index()\n",
        "    repurchase_gaps['penalty_gap'] = np.where(\n",
        "        repurchase_gaps['months_until_next'] > 3, 0.6,  # Long gaps = penalty\n",
        "        np.where(repurchase_gaps['months_until_next'].isna(), 0.4,  # Never repurchased = strong penalty\n",
        "        1.0)  # Quick repurchase = no penalty\n",
        "    )\n",
        "else:\n",
        "    repurchase_gaps = pd.DataFrame({'sku_id': [], 'penalty_gap': []})\n",
        "\n",
        "# Combine all penalties\n",
        "freq_tbl = freq_tbl.merge(repeat_purchase_rate[['sku_id', 'penalty_repeat']], on='sku_id', how='left')\n",
        "freq_tbl = freq_tbl.merge(glob_pop[['sku_id', 'penalty_popularity']], on='sku_id', how='left')\n",
        "\n",
        "if len(repurchase_gaps) > 0:\n",
        "    freq_tbl = freq_tbl.merge(repurchase_gaps[['sku_id', 'penalty_gap']], on='sku_id', how='left')\n",
        "    freq_tbl['penalty_gap'] = freq_tbl['penalty_gap'].fillna(0.5)  # Default penalty if no data\n",
        "else:\n",
        "    freq_tbl['penalty_gap'] = 1.0\n",
        "\n",
        "freq_tbl['penalty_repeat'] = freq_tbl['penalty_repeat'].fillna(1.0)\n",
        "freq_tbl['penalty_popularity'] = freq_tbl['penalty_popularity'].fillna(1.0)\n",
        "\n",
        "# Combined penalty (multiply all penalties)\n",
        "freq_tbl['total_penalty'] = freq_tbl['penalty_repeat'] * freq_tbl['penalty_popularity'] * freq_tbl['penalty_gap']\n",
        "\n",
        "test_customers = test_purchases['customer_id'].unique()\n",
        "recommendations = []\n",
        "\n",
        "for customer_id in test_customers:\n",
        "    customer_freq = freq_tbl[freq_tbl['customer_id'] == customer_id].copy()\n",
        "    \n",
        "    if len(customer_freq) > 0:\n",
        "        # Apply combined penalty to filter out bad recommendations\n",
        "        customer_freq['score'] = (customer_freq['cnt'] + 0.05 * customer_freq['glob_score']) * customer_freq['total_penalty']\n",
        "        recommended_skus = customer_freq.nlargest(12, 'score')['sku_id'].tolist()[:10]  # Get top 12, take best 10\n",
        "    else:\n",
        "        # For new customers, use items with good repeat rates\n",
        "        good_global = glob_pop.merge(repeat_purchase_rate[['sku_id', 'penalty_repeat']], on='sku_id')\n",
        "        good_global = good_global[good_global['penalty_repeat'] > 0.8]  # Only items with good repeat rates\n",
        "        if len(good_global) >= 10:\n",
        "            recommended_skus = good_global.nlargest(10, 'glob_score')['sku_id'].tolist()\n",
        "        else:\n",
        "            recommended_skus = glob_pop.head(10)['sku_id'].tolist()\n",
        "    \n",
        "    if len(recommended_skus) < 10:\n",
        "        remaining_skus = [sku for sku in glob_pop['sku_id'].tolist() if sku not in recommended_skus]\n",
        "        recommended_skus.extend(remaining_skus[:10-len(recommended_skus)])\n",
        "    \n",
        "    recommendations.append([customer_id] + recommended_skus[:10])\n",
        "\n",
        "columns = ['customer_id'] + [f'sku_id_{i}' for i in range(1, 11)]\n",
        "submission_df = pd.DataFrame(recommendations, columns=columns)\n",
        "submission_df.to_csv(\"commerce_challenge2_predictions_Ar2.csv\", index=False)\n",
        "\n",
        "print(f\"âœ… Enhanced anti-bad v2 predictions saved: {submission_df.shape[0]} customers\")\n",
        "print(\"ðŸŽ¯ Expected: 0.0775 - 0.0785\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are adding a rank based approach and enhanced features. This builds upon the anti-recommendation strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Enhanced Rank-Based for 0.085 target...\n",
            "ðŸ”„ Fast co-purchase analysis...\n",
            "âœ… Enhanced rank predictions saved: 500 customers\n",
            "ðŸŽ¯ Expected: 0.079 - 0.082\n"
          ]
        }
      ],
      "source": [
        "# ENHANCED RANK-BASED + SMART FEATURES\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_purchases = pd.read_csv(\"../Commerce/purchases_train.csv\")\n",
        "test_purchases = pd.read_csv(\"../Commerce/purchases_test.csv\")\n",
        "products_sub = pd.read_csv(\"/Users/amolighatimma/Desktop/AgentDS/Commerce/products_sub.csv\")\n",
        "\n",
        "valid_skus = set(products_sub['sku_id'].tolist())\n",
        "train_filtered = train_purchases[train_purchases['sku_id'].isin(valid_skus)]\n",
        "\n",
        "print(\"ðŸš€ Enhanced Rank-Based for 0.085 target...\")\n",
        "\n",
        "# Your working frequency approach\n",
        "freq_tbl = (train_filtered.groupby([\"customer_id\", \"sku_id\"])\n",
        "            .size().rename(\"cnt\").reset_index())\n",
        "\n",
        "# Enhanced global popularity with multiple rank-based strategies\n",
        "glob_pop = (train_filtered[\"sku_id\"].value_counts()\n",
        "            .rename_axis(\"sku_id\").rename(\"glob_cnt\").reset_index())\n",
        "\n",
        "# Multiple rank-based scoring strategies\n",
        "glob_pop['rank'] = glob_pop['glob_cnt'].rank(method='dense', ascending=False)\n",
        "glob_pop['score_rank_inv_sqrt'] = 1.0 / np.sqrt(glob_pop['rank'])  # Your working one\n",
        "glob_pop['score_rank_inv_log'] = 1.0 / np.log(glob_pop['rank'] + 1)  # Alternative\n",
        "glob_pop['score_rank_power'] = 1.0 / (glob_pop['rank'] ** 0.7)  # Power law\n",
        "\n",
        "# Blend the best performing rank-based scores\n",
        "glob_pop['glob_score'] = (\n",
        "    glob_pop['score_rank_inv_sqrt'] * 0.6 + \n",
        "    glob_pop['score_rank_inv_log'] * 0.3 +\n",
        "    glob_pop['score_rank_power'] * 0.1\n",
        ")\n",
        "\n",
        "# SMART FEATURE 1: Customer purchase patterns\n",
        "customer_stats = train_filtered.groupby('customer_id').agg({\n",
        "    'sku_id': ['count', 'nunique']\n",
        "}).reset_index()\n",
        "customer_stats.columns = ['customer_id', 'total_purchases', 'unique_products']\n",
        "customer_stats['repeat_ratio'] = customer_stats['total_purchases'] / customer_stats['unique_products']\n",
        "\n",
        "# SMART FEATURE 2: Item quality (repeat purchase rate)\n",
        "item_quality = train_filtered.groupby('sku_id').agg({\n",
        "    'customer_id': ['count', 'nunique']\n",
        "}).reset_index()\n",
        "item_quality.columns = ['sku_id', 'total_purchases', 'unique_customers']\n",
        "item_quality['repeat_rate'] = item_quality['total_purchases'] / item_quality['unique_customers']\n",
        "item_quality['quality_score'] = np.sqrt(item_quality['total_purchases']) * item_quality['repeat_rate']\n",
        "\n",
        "# SMART FEATURE 3: Simple co-purchase (fast version)\n",
        "print(\"ðŸ”„ Fast co-purchase analysis...\")\n",
        "customer_items = train_filtered.groupby('customer_id')['sku_id'].apply(list)\n",
        "multi_purchase = customer_items[customer_items.apply(len) > 1]\n",
        "\n",
        "co_purchase_counts = {}\n",
        "for items in multi_purchase:\n",
        "    for i in range(len(items)):\n",
        "        for j in range(i+1, len(items)):\n",
        "            pair = (items[i], items[j])\n",
        "            co_purchase_counts[pair] = co_purchase_counts.get(pair, 0) + 1\n",
        "\n",
        "# Convert to item->item mapping\n",
        "co_purchase_map = {}\n",
        "for (item1, item2), count in co_purchase_counts.items():\n",
        "    if item1 not in co_purchase_map:\n",
        "        co_purchase_map[item1] = {}\n",
        "    if item2 not in co_purchase_map:\n",
        "        co_purchase_map[item2] = {}\n",
        "    co_purchase_map[item1][item2] = count\n",
        "    co_purchase_map[item2][item1] = count\n",
        "\n",
        "# Merge all features\n",
        "freq_tbl = freq_tbl.merge(customer_stats, on='customer_id')\n",
        "freq_tbl = freq_tbl.merge(glob_pop[['sku_id', 'glob_score']], on='sku_id', how='left')\n",
        "freq_tbl = freq_tbl.merge(item_quality[['sku_id', 'quality_score']], on='sku_id', how='left')\n",
        "freq_tbl['glob_score'] = freq_tbl['glob_score'].fillna(0)\n",
        "freq_tbl['quality_score'] = freq_tbl['quality_score'].fillna(0)\n",
        "\n",
        "test_customers = test_purchases['customer_id'].unique()\n",
        "recommendations = []\n",
        "\n",
        "for customer_id in test_customers:\n",
        "    customer_freq = freq_tbl[freq_tbl['customer_id'] == customer_id].copy()\n",
        "    customer_items = customer_freq['sku_id'].tolist()\n",
        "    \n",
        "    if len(customer_freq) > 0:\n",
        "        total_purchases = customer_freq['total_purchases'].iloc[0]\n",
        "        repeat_ratio = customer_freq['repeat_ratio'].iloc[0]\n",
        "        \n",
        "        # ENHANCED SCORING with customer segmentation\n",
        "        if total_purchases >= 15:\n",
        "            # Super users: Strong personalization + quality focus\n",
        "            customer_freq['score'] = (\n",
        "                customer_freq['cnt'] * 2.5 +           # Very strong personal\n",
        "                customer_freq['quality_score'] * 0.8 + # High quality focus\n",
        "                customer_freq['glob_score'] * 0.02     # Minimal global\n",
        "            )\n",
        "        elif total_purchases >= 8:\n",
        "            # Heavy users: Balanced with quality\n",
        "            customer_freq['score'] = (\n",
        "                customer_freq['cnt'] * 2.0 +\n",
        "                customer_freq['quality_score'] * 0.5 +\n",
        "                customer_freq['glob_score'] * 0.05\n",
        "            )\n",
        "        elif total_purchases >= 3:\n",
        "            # Medium users: Standard\n",
        "            customer_freq['score'] = (\n",
        "                customer_freq['cnt'] * 1.5 +\n",
        "                customer_freq['quality_score'] * 0.3 +\n",
        "                customer_freq['glob_score'] * 0.08\n",
        "            )\n",
        "        else:\n",
        "            # Light users: More global + quality\n",
        "            customer_freq['score'] = (\n",
        "                customer_freq['cnt'] * 1.2 +\n",
        "                customer_freq['quality_score'] * 0.2 +\n",
        "                customer_freq['glob_score'] * 0.12\n",
        "            )\n",
        "        \n",
        "        # ADD CO-PURCHASE BOOST (limited to avoid overfitting)\n",
        "        co_boost_added = 0\n",
        "        for item in customer_items[:5]:  # Only top 5 purchased items\n",
        "            if item in co_purchase_map and co_boost_added < 3:  # Limit to 3 boosts\n",
        "                for co_item, count in list(co_purchase_map[item].items())[:3]:  # Top 3 co-purchases\n",
        "                    if co_item not in customer_items and count >= 2:  # Only strong co-purchases\n",
        "                        if co_item in customer_freq['sku_id'].values:\n",
        "                            customer_freq.loc[customer_freq['sku_id'] == co_item, 'score'] += count * 0.15\n",
        "                        else:\n",
        "                            # Only add if it's a strong co-purchase and we have room\n",
        "                            if co_boost_added < 2:\n",
        "                                new_score = count * 0.15\n",
        "                                # Add quality and global if available\n",
        "                                quality = item_quality[item_quality['sku_id'] == co_item]['quality_score'].iloc[0] if len(item_quality[item_quality['sku_id'] == co_item]) > 0 else 0\n",
        "                                global_score = glob_pop[glob_pop['sku_id'] == co_item]['glob_score'].iloc[0] if len(glob_pop[glob_pop['sku_id'] == co_item]) > 0 else 0\n",
        "                                new_score += quality * 0.2 + global_score * 0.05\n",
        "                                \n",
        "                                new_row = {\n",
        "                                    'customer_id': customer_id, \n",
        "                                    'sku_id': co_item, \n",
        "                                    'cnt': 0,\n",
        "                                    'total_purchases': total_purchases,\n",
        "                                    'unique_products': customer_freq['unique_products'].iloc[0],\n",
        "                                    'repeat_ratio': repeat_ratio,\n",
        "                                    'glob_score': global_score,\n",
        "                                    'quality_score': quality,\n",
        "                                    'score': new_score\n",
        "                                }\n",
        "                                customer_freq = pd.concat([customer_freq, pd.DataFrame([new_row])], ignore_index=True)\n",
        "                                co_boost_added += 1\n",
        "        \n",
        "        recommended_skus = customer_freq.nlargest(12, 'score')['sku_id'].tolist()[:10]\n",
        "        \n",
        "    else:\n",
        "        # New customer: High-quality global items\n",
        "        high_quality_global = glob_pop.merge(item_quality[['sku_id', 'quality_score']], on='sku_id')\n",
        "        high_quality_global['score'] = (\n",
        "            high_quality_global['glob_score'] * 1.2 + \n",
        "            high_quality_global['quality_score'] * 0.8\n",
        "        )\n",
        "        recommended_skus = high_quality_global.nlargest(10, 'score')['sku_id'].tolist()\n",
        "    \n",
        "    # Ensure exactly 10\n",
        "    if len(recommended_skus) < 10:\n",
        "        remaining_skus = [sku for sku in glob_pop['sku_id'].tolist() if sku not in recommended_skus]\n",
        "        recommended_skus.extend(remaining_skus[:10-len(recommended_skus)])\n",
        "    \n",
        "    recommendations.append([customer_id] + recommended_skus[:10])\n",
        "\n",
        "columns = ['customer_id'] + [f'sku_id_{i}' for i in range(1, 11)]\n",
        "submission_df = pd.DataFrame(recommendations, columns=columns)\n",
        "submission_df.to_csv(\"commerce_challenge2_predictions_er.csv\", index=False)\n",
        "\n",
        "print(f\"âœ… Enhanced rank predictions saved: {submission_df.shape[0]} customers\")\n",
        "print(\"ðŸŽ¯ Expected: 0.079 - 0.082\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To push up a few points in our metric, we tried to ensure data leakage was accounted for producing the best model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ” Data Leakage Approach - Finding Hidden Patterns...\n",
            "Analyzing successful patterns...\n",
            "Overlap customers: 500, New customers: 0\n",
            "Universal items (bought by many customers): 164\n",
            "High conversion items: 13\n",
            "âœ… Data leakage approach saved: 500 customers\n",
            "ðŸŽ¯ This might find patterns others are missing\n"
          ]
        }
      ],
      "source": [
        "# DATA LEAKAGE APPROACH - FIND HIDDEN PATTERNS\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "train_purchases = pd.read_csv(\"../Commerce/purchases_train.csv\")\n",
        "test_purchases = pd.read_csv(\"../Commerce/purchases_test.csv\")\n",
        "products_sub = pd.read_csv(\"/Users/amolighatimma/Desktop/AgentDS/Commerce/products_sub.csv\")\n",
        "\n",
        "valid_skus = set(products_sub['sku_id'].tolist())\n",
        "train_filtered = train_purchases[train_purchases['sku_id'].isin(valid_skus)]\n",
        "\n",
        "print(\"ðŸ” Data Leakage Approach - Finding Hidden Patterns...\")\n",
        "\n",
        "# Let's analyze what makes the 0.0802 model work\n",
        "print(\"Analyzing successful patterns...\")\n",
        "\n",
        "# Strategy 1: Look for customers who behave similarly in train and test\n",
        "train_customers = set(train_filtered['customer_id'])\n",
        "test_customers_list = test_purchases['customer_id'].unique()\n",
        "\n",
        "overlap_customers = [c for c in test_customers_list if c in train_customers]\n",
        "new_customers = [c for c in test_customers_list if c not in train_customers]\n",
        "\n",
        "print(f\"Overlap customers: {len(overlap_customers)}, New customers: {len(new_customers)}\")\n",
        "\n",
        "# Strategy 2: Find items that are \"universal winners\" - bought by many different customer types\n",
        "customer_item_matrix = train_filtered.groupby(['customer_id', 'sku_id']).size().unstack(fill_value=0)\n",
        "item_customer_count = (customer_item_matrix > 0).sum(axis=0)\n",
        "universal_items = item_customer_count[item_customer_count >= item_customer_count.quantile(0.8)].index.tolist()\n",
        "\n",
        "print(f\"Universal items (bought by many customers): {len(universal_items)}\")\n",
        "\n",
        "# Strategy 3: Find items with high \"conversion rates\" - once bought, often bought again\n",
        "item_stats = train_filtered.groupby('sku_id').agg({\n",
        "    'customer_id': ['count', 'nunique']\n",
        "}).reset_index()\n",
        "item_stats.columns = ['sku_id', 'total_purchases', 'unique_customers']\n",
        "item_stats['repeat_rate'] = item_stats['total_purchases'] / item_stats['unique_customers']\n",
        "high_conversion_items = item_stats[item_stats['repeat_rate'] >= 1.5]['sku_id'].tolist()\n",
        "\n",
        "print(f\"High conversion items: {len(high_conversion_items)}\")\n",
        "\n",
        "# Build recommendations using these insights\n",
        "freq_tbl = (train_filtered.groupby([\"customer_id\", \"sku_id\"])\n",
        "            .size().rename(\"cnt\").reset_index())\n",
        "\n",
        "glob_pop = (train_filtered[\"sku_id\"].value_counts()\n",
        "            .rename_axis(\"sku_id\").rename(\"glob_cnt\").reset_index())\n",
        "glob_pop['rank'] = glob_pop['glob_cnt'].rank(method='dense', ascending=False)\n",
        "glob_pop['glob_score'] = 1.0 / np.sqrt(glob_pop['rank'])\n",
        "\n",
        "# Add our new insights\n",
        "freq_tbl = freq_tbl.merge(glob_pop[['sku_id', 'glob_score']], on='sku_id', how='left')\n",
        "freq_tbl['glob_score'] = freq_tbl['glob_score'].fillna(0)\n",
        "\n",
        "# Mark universal and high-conversion items\n",
        "freq_tbl['is_universal'] = freq_tbl['sku_id'].isin(universal_items)\n",
        "freq_tbl['is_high_conversion'] = freq_tbl['sku_id'].isin(high_conversion_items)\n",
        "\n",
        "test_customers = test_purchases['customer_id'].unique()\n",
        "recommendations = []\n",
        "\n",
        "for customer_id in test_customers:\n",
        "    customer_freq = freq_tbl[freq_tbl['customer_id'] == customer_id].copy()\n",
        "    \n",
        "    if len(customer_freq) > 0:\n",
        "        # INNOVATIVE SCORING: Leverage our new insights\n",
        "        customer_freq['score'] = (\n",
        "            customer_freq['cnt'] * 2.0 +                    # Personal history\n",
        "            customer_freq['glob_score'] * 0.08 +           # Global rank\n",
        "            customer_freq['is_universal'] * 0.5 +          # Universal items boost\n",
        "            customer_freq['is_high_conversion'] * 0.7      # High conversion boost\n",
        "        )\n",
        "        \n",
        "        # For customers with purchase history, also consider what similar customers bought\n",
        "        customer_items = customer_freq['sku_id'].tolist()\n",
        "        \n",
        "        # Find customers with similar purchase patterns\n",
        "        similar_customers = train_filtered[\n",
        "            train_filtered['sku_id'].isin(customer_items) & \n",
        "            (train_filtered['customer_id'] != customer_id)\n",
        "        ]['customer_id'].unique()\n",
        "        \n",
        "        if len(similar_customers) > 0:\n",
        "            similar_purchases = train_filtered[\n",
        "                train_filtered['customer_id'].isin(similar_customers) & \n",
        "                (~train_filtered['sku_id'].isin(customer_items))\n",
        "            ]\n",
        "            \n",
        "            if len(similar_purchases) > 0:\n",
        "                similar_items = similar_purchases['sku_id'].value_counts().head(3)\n",
        "                for item, count in similar_items.items():\n",
        "                    if item not in customer_freq['sku_id'].values and item in valid_skus:\n",
        "                        # Check if this is a universal or high-conversion item\n",
        "                        is_uni = item in universal_items\n",
        "                        is_conv = item in high_conversion_items\n",
        "                        boost = count * 0.3 + is_uni * 0.3 + is_conv * 0.4\n",
        "                        \n",
        "                        new_row = {\n",
        "                            'customer_id': customer_id, \n",
        "                            'sku_id': item, \n",
        "                            'cnt': 0,\n",
        "                            'glob_score': glob_pop[glob_pop['sku_id'] == item]['glob_score'].iloc[0] if len(glob_pop[glob_pop['sku_id'] == item]) > 0 else 0,\n",
        "                            'is_universal': is_uni,\n",
        "                            'is_high_conversion': is_conv,\n",
        "                            'score': boost\n",
        "                        }\n",
        "                        customer_freq = pd.concat([customer_freq, pd.DataFrame([new_row])], ignore_index=True)\n",
        "        \n",
        "        recommended_skus = customer_freq.nlargest(10, 'score')['sku_id'].tolist()\n",
        "    else:\n",
        "        # New customer strategy: focus on universal + high conversion items\n",
        "        new_cust_scores = glob_pop.copy()\n",
        "        new_cust_scores['is_universal'] = new_cust_scores['sku_id'].isin(universal_items)\n",
        "        new_cust_scores['is_high_conversion'] = new_cust_scores['sku_id'].isin(high_conversion_items)\n",
        "        new_cust_scores['score'] = (\n",
        "            new_cust_scores['glob_score'] * 0.6 +\n",
        "            new_cust_scores['is_universal'] * 0.8 +\n",
        "            new_cust_scores['is_high_conversion'] * 1.0\n",
        "        )\n",
        "        recommended_skus = new_cust_scores.nlargest(10, 'score')['sku_id'].tolist()\n",
        "    \n",
        "    # Ensure exactly 10\n",
        "    if len(recommended_skus) < 10:\n",
        "        remaining_skus = [sku for sku in glob_pop['sku_id'].tolist() if sku not in recommended_skus]\n",
        "        recommended_skus.extend(remaining_skus[:10-len(recommended_skus)])\n",
        "    \n",
        "    recommendations.append([customer_id] + recommended_skus[:10])\n",
        "\n",
        "columns = ['customer_id'] + [f'sku_id_{i}' for i in range(1, 11)]\n",
        "submission_df = pd.DataFrame(recommendations, columns=columns)\n",
        "submission_df.to_csv(\"commerce_challenge2_predictions.csv\", index=False)\n",
        "\n",
        "print(f\"âœ… Data leakage approach saved: {submission_df.shape[0]} customers\")\n",
        "print(\"ðŸŽ¯ This might find patterns others are missing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the below existing submit prediction code was run after trying a model that was not the best. However, if you run the documented code directly above(data leakage approach), it should have a NDCG of 0.804."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Submitting predictions...\n",
            "âœ… Prediction submitted successfully!\n",
            "ðŸ“Š Score: 0.0800 (NDCG@10)\n",
            "âœ… Validation passed\n",
            "âœ… Submission successful!\n",
            "   ðŸ“Š Score: 0.0800\n",
            "   ðŸ“ Metric: NDCG@10\n",
            "   âœ”ï¸  Validation: Passed\n",
            "\n",
            "ðŸŽ¯ Next steps:\n",
            "   1. Try incorporating relevant information outside this table!\n",
            "   2. Move on to Commerce Challenge 3!\n"
          ]
        }
      ],
      "source": [
        "# 3. Submit Predictions\n",
        "\n",
        "# Submit predictions to the competition\n",
        "print(\"ðŸš€ Submitting predictions...\")\n",
        "\n",
        "try:\n",
        "    result = client.submit_prediction(\"Commerce\", 2, \"commerce_challenge2_predictions.csv\")\n",
        "    \n",
        "    if result['success']:\n",
        "        print(\"âœ… Submission successful!\")\n",
        "        print(f\"   ðŸ“Š Score: {result['score']:.4f}\")\n",
        "        print(f\"   ðŸ“ Metric: {result['metric_name']}\")\n",
        "        print(f\"   âœ”ï¸  Validation: {'Passed' if result['validation_passed'] else 'Failed'}\")\n",
        "    else:\n",
        "        print(\"âŒ Submission failed!\")\n",
        "        print(f\"   Error details: {result.get('details', {}).get('validation_errors', 'Unknown error')}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"ðŸ’¥ Submission error: {e}\")\n",
        "    print(\"ðŸ”§ Check your API key and team name are correct!\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ Next steps:\")\n",
        "print(\"   1. Try incorporating relevant information outside this table!\")\n",
        "print(\"   2. Move on to Commerce Challenge 3!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "agentds_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
